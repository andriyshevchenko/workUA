"""–¢–µ—Å—Ç remote —Ñ—ñ–ª—å—Ç—Ä–∞ –¥–ª—è Work.ua"""
import asyncio
from scraper import WorkUAScraper
from utils import separator_line


async def test_remote_search():
    """–¢–µ—Å—Ç—É—î–º–æ –ø–æ—à—É–∫ –¥–∏—Å—Ç–∞–Ω—Ü—ñ–π–Ω–∏—Ö –≤–∞–∫–∞–Ω—Å—ñ–π"""
    
    scraper = WorkUAScraper()
    await scraper.start(headless=False)
    
    print("\n" + separator_line())
    print("üß™ –¢–ï–°–¢: –ü–æ—à—É–∫ –î–ò–°–¢–ê–ù–¶–Ü–ô–ù–ò–• –≤–∞–∫–∞–Ω—Å—ñ–π (remote=True)")
    print(separator_line() + "\n")
    
    jobs = await scraper.search_jobs(
        keyword="python developer",
        remote=True,
        max_pages=1
    )
    
    print(f"\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(jobs)} –¥–∏—Å—Ç–∞–Ω—Ü—ñ–π–Ω–∏—Ö –≤–∞–∫–∞–Ω—Å—ñ–π:\n")
    
    for i, job in enumerate(jobs[:5], 1):  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 5
        print(f"{i}. {job.title}")
        print(f"   üè¢ {job.company}")
        print(f"   üìç {job.location}")
        print(f"   üîó {job.url}")
        print()
    
    print("\n" + separator_line())
    print("üß™ –¢–ï–°–¢: –ü–æ—à—É–∫ –ó–í–ò–ß–ê–ô–ù–ò–• –≤–∞–∫–∞–Ω—Å—ñ–π (remote=False)")
    print(separator_line() + "\n")
    
    jobs_normal = await scraper.search_jobs(
        keyword="python developer",
        location="–ö–∏—ó–≤",
        remote=False,
        max_pages=1
    )
    
    print(f"\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(jobs_normal)} –∑–≤–∏—á–∞–π–Ω–∏—Ö –≤–∞–∫–∞–Ω—Å—ñ–π:\n")
    
    for i, job in enumerate(jobs_normal[:5], 1):
        print(f"{i}. {job.title}")
        print(f"   üè¢ {job.company}")
        print(f"   üìç {job.location}")
        print(f"   üîó {job.url}")
        print()
    
    await scraper.close()
    
    print("\n" + separator_line())
    print("‚ú® –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù–û")
    print(separator_line())


if __name__ == "__main__":
    asyncio.run(test_remote_search())
