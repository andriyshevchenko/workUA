"""LangGraph AI –∞–≥–µ–Ω—Ç –¥–ª—è —Ä–æ–∑—É–º–Ω–æ–≥–æ –≤—ñ–¥–≥—É–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å—ñ—ó"""
from typing import TypedDict, Annotated, List, Optional
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from scraper import JobListing, WorkUAScraper
from config import config
import json


class AgentState(TypedDict):
    """–°—Ç–∞–Ω –∞–≥–µ–Ω—Ç–∞"""
    resume_text: str  # –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    search_keywords: List[str]  # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ—à—É–∫—É
    locations: List[str]  # –õ–æ–∫–∞—Ü—ñ—ó
    found_jobs: List[JobListing]  # –ó–Ω–∞–π–¥–µ–Ω—ñ –≤–∞–∫–∞–Ω—Å—ñ—ó
    analyzed_jobs: List[dict]  # –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –≤–∞–∫–∞–Ω—Å—ñ—ó –∑ –æ—Ü—ñ–Ω–∫–∞–º–∏
    applied_jobs: List[JobListing]  # –í–∞–∫–∞–Ω—Å—ñ—ó –Ω–∞ —è–∫—ñ –≤—ñ–¥–≥—É–∫–Ω—É–ª–∏—Å—å
    rejected_jobs: List[JobListing]  # –í—ñ–¥—Ö–∏–ª–µ–Ω—ñ –≤–∞–∫–∞–Ω—Å—ñ—ó
    current_job_index: int  # –ü–æ—Ç–æ—á–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –≤–∞–∫–∞–Ω—Å—ñ—ó
    error: Optional[str]  # –ü–æ–º–∏–ª–∫–∏ —è–∫—â–æ —î
    scraper: Optional[WorkUAScraper]  # –ï–∫–∑–µ–º–ø–ª—è—Ä scraper


class WorkUAAgent:
    """AI –ê–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó —Ä–æ–∑—Å–∏–ª–∫–∏ —Ä–µ–∑—é–º–µ"""
    
    def __init__(self):
        self.llm = self._init_llm()
        self.graph = self._build_graph()
        
    def _init_llm(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ LLM"""
        if config.OPENAI_API_KEY:
            return ChatOpenAI(
                model=config.MODEL_NAME,
                temperature=config.TEMPERATURE,
                api_key=config.OPENAI_API_KEY
            )
        else:
            raise ValueError("–ü–æ—Ç—Ä—ñ–±–µ–Ω OPENAI_API_KEY")
            
    def _build_graph(self) -> StateGraph:
        """–ü–æ–±—É–¥—É–≤–∞—Ç–∏ LangGraph workflow"""
        workflow = StateGraph(AgentState)
        
        # –î–æ–¥–∞—Ç–∏ –Ω–æ–¥–∏
        workflow.add_node("load_resume", self.load_resume_node)
        workflow.add_node("search_jobs", self.search_jobs_node)
        workflow.add_node("analyze_job", self.analyze_job_node)
        workflow.add_node("apply_job", self.apply_job_node)
        workflow.add_node("finalize", self.finalize_node)
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –ø–æ—Ç—ñ–∫
        workflow.add_edge(START, "load_resume")
        workflow.add_edge("load_resume", "search_jobs")
        workflow.add_edge("search_jobs", "analyze_job")
        
        # –£–º–æ–≤–Ω–∏–π –ø–µ—Ä–µ—Ö—ñ–¥ –ø—ñ—Å–ª—è –∞–Ω–∞–ª—ñ–∑—É
        workflow.add_conditional_edges(
            "analyze_job",
            self.should_continue_analyzing,
            {
                "apply": "apply_job",
                "skip": "analyze_job",
                "done": "finalize"
            }
        )
        
        workflow.add_edge("apply_job", "analyze_job")
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
        
    # ============ NODES ============
    
    async def load_resume_node(self, state: AgentState) -> AgentState:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ä–µ–∑—é–º–µ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"""
        print("üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—é–º–µ...")
        
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ —á–∏—Ç–∞–Ω–Ω—è PDF/DOCX
        # –ü–æ–∫–∏ —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ placeholder
        resume_text = """
        –î–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π Python —Ä–æ–∑—Ä–æ–±–Ω–∏–∫ –∑ 3+ —Ä–æ–∫–∞–º–∏ –¥–æ—Å–≤—ñ–¥—É.
        –ù–∞–≤–∏—á–∫–∏: Python, Django, FastAPI, PostgreSQL, Docker, AWS.
        –î–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ AI/ML –ø—Ä–æ–µ–∫—Ç–∞–º–∏, REST API, –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥.
        """
        
        state["resume_text"] = resume_text
        state["current_job_index"] = 0
        state["applied_jobs"] = []
        state["rejected_jobs"] = []
        state["analyzed_jobs"] = []
        
        print("‚úì –†–µ–∑—é–º–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return state
        
    async def search_jobs_node(self, state: AgentState) -> AgentState:
        """–ü–æ—à—É–∫ –≤–∞–∫–∞–Ω—Å—ñ–π"""
        print("üîç –ü–æ—à—É–∫ –≤–∞–∫–∞–Ω—Å—ñ–π...")
        
        scraper = WorkUAScraper()
        await scraper.start(headless=config.HEADLESS)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é
        is_logged_in = await scraper.check_login_status()
        if not is_logged_in:
            print("‚ö†Ô∏è –£–í–ê–ì–ê: –ù–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ. –í—ñ–¥–≥—É–∫–∏ –º–æ–∂—É—Ç—å –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏.")
            print("–ó–∞–ø—É—Å—Ç—ñ—Ç—å explorer.py –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó")
            
        all_jobs = []
        
        # –ü–æ—à—É–∫ –∑–∞ –∫–æ–∂–Ω–∏–º –∫–ª—é—á–æ–≤–∏–º —Å–ª–æ–≤–æ–º
        for keyword in state["search_keywords"]:
            if config.REMOTE_ONLY:
                # –Ø–∫—â–æ —à—É–∫–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ remote, —ñ–≥–Ω–æ—Ä—É—î–º–æ locations
                jobs = await scraper.search_jobs(
                    keyword=keyword,
                    remote=True,
                    max_pages=2
                )
                all_jobs.extend(jobs)
            else:
                # –ó–≤–∏—á–∞–π–Ω–∏–π –ø–æ—à—É–∫ –∑ locations
                for location in state.get("locations", [None]):
                    jobs = await scraper.search_jobs(
                        keyword=keyword,
                        location=location,
                        max_pages=2
                    )
                    all_jobs.extend(jobs)
                
        # –í–∏–¥–∞–ª–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ URL
        unique_jobs = {job.url: job for job in all_jobs}.values()
        state["found_jobs"] = list(unique_jobs)
        state["scraper"] = scraper
        
        print(f"‚úì –ó–Ω–∞–π–¥–µ–Ω–æ {len(state['found_jobs'])} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –≤–∞–∫–∞–Ω—Å—ñ–π")
        return state
        
    async def analyze_job_node(self, state: AgentState) -> AgentState:
        """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∫–∞–Ω—Å—ñ—ó —á–µ—Ä–µ–∑ LLM"""
        idx = state["current_job_index"]
        
        if idx >= len(state["found_jobs"]):
            return state
            
        job = state["found_jobs"][idx]
        print(f"\nü§î –ê–Ω–∞–ª—ñ–∑ –≤–∞–∫–∞–Ω—Å—ñ—ó [{idx + 1}/{len(state['found_jobs'])}]: {job.title}")
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ–≤–Ω—ñ –¥–µ—Ç–∞–ª—ñ –≤–∞–∫–∞–Ω—Å—ñ—ó
        scraper = state["scraper"]
        job = await scraper.get_job_details(job)
        
        # –ê–Ω–∞–ª—ñ–∑ —á–µ—Ä–µ–∑ LLM
        analysis = await self._llm_analyze_job(state["resume_text"], job)
        
        state["analyzed_jobs"].append({
            "job": job,
            "score": analysis["score"],
            "reason": analysis["reason"],
            "should_apply": analysis["should_apply"]
        })
        
        print(f"üìä –û—Ü—ñ–Ω–∫–∞: {analysis['score']}/10")
        print(f"üí≠ –ü—Ä–∏—á–∏–Ω–∞: {analysis['reason']}")
        
        if not analysis["should_apply"]:
            state["rejected_jobs"].append(job)
            print("‚ùå –í–∞–∫–∞–Ω—Å—ñ—è –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
            
        return state
        
    async def apply_job_node(self, state: AgentState) -> AgentState:
        """–í—ñ–¥–≥—É–∫–Ω—É—Ç–∏—Å—è –Ω–∞ –≤–∞–∫–∞–Ω—Å—ñ—é"""
        idx = state["current_job_index"]
        job = state["found_jobs"][idx]
        
        print(f"üì§ –í—ñ–¥–≥—É–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å—ñ—é: {job.title}")
        
        scraper = state["scraper"]
        success = await scraper.apply_to_job(job)
        
        if success:
            state["applied_jobs"].append(job)
            print("‚úÖ –£—Å–ø—ñ—à–Ω–æ –≤—ñ–¥–≥—É–∫–Ω—É–ª–∏—Å—å!")
        else:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–≥—É–∫–Ω—É—Ç–∏—Å—å")
            
        return state
        
    async def finalize_node(self, state: AgentState) -> AgentState:
        """–§—ñ–Ω–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç"""
        print("\n" + "="*60)
        print("üìä –§–Ü–ù–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢")
        print("="*60)
        
        print(f"\n‚úÖ –£—Å–ø—ñ—à–Ω–æ –≤—ñ–¥–≥—É–∫–Ω—É–ª–∏—Å—å: {len(state['applied_jobs'])}")
        for job in state["applied_jobs"]:
            print(f"   - {job.title} –≤ {job.company}")
            
        print(f"\n‚ùå –í—ñ–¥—Ö–∏–ª–µ–Ω–æ: {len(state['rejected_jobs'])}")
        for job in state["rejected_jobs"][:5]:
            print(f"   - {job.title} –≤ {job.company}")
            
        print(f"\nüìà –í—Å—å–æ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {len(state['analyzed_jobs'])}")
        print(f"üìä –°–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {len(state['applied_jobs'])}/{len(state['found_jobs'])}")
        
        # –ó–∞–∫—Ä–∏—Ç–∏ scraper
        scraper = state["scraper"]
        if scraper:
            await scraper.close()
            
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç
        self._save_report(state)
        
        return state
        
    # ============ HELPERS ============
    
    def should_continue_analyzing(self, state: AgentState) -> str:
        """–í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫ –ø—ñ—Å–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        idx = state["current_job_index"]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î —â–µ –≤–∞–∫–∞–Ω—Å—ñ—ó
        if idx >= len(state["found_jobs"]):
            return "done"
            
        # –û—Ç—Ä–∏–º–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
        if state["analyzed_jobs"]:
            last_analysis = state["analyzed_jobs"][-1]
            state["current_job_index"] += 1
            
            if last_analysis["should_apply"]:
                return "apply"
            else:
                return "skip"
                
        return "done"
        
    async def _llm_analyze_job(self, resume: str, job: JobListing) -> dict:
        """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∫–∞–Ω—Å—ñ—ó —á–µ—Ä–µ–∑ LLM"""
        
        system_prompt = """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –ø—ñ–¥–±–æ—Ä—É –≤–∞–∫–∞–Ω—Å—ñ–π. –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–∞–∫–∞–Ω—Å—ñ—è –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç—É.

–û—Ü—ñ–Ω—ñ—Ç—å –∑–∞ —à–∫–∞–ª–æ—é –≤—ñ–¥ 1 –¥–æ 10, –¥–µ:
- 1-3: –ó–æ–≤—Å—ñ–º –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å
- 4-6: –ß–∞—Å—Ç–∫–æ–≤–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å
- 7-8: –î–æ–±—Ä–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å
- 9-10: –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON:
{
    "score": <—á–∏—Å–ª–æ –≤—ñ–¥ 1 –¥–æ 10>,
    "reason": "<–∫–æ—Ä–æ—Ç–∫–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é>",
    "should_apply": <true –∞–±–æ false>
}

–ö—Ä–∏—Ç–µ—Ä—ñ—ó –æ—Ü—ñ–Ω–∫–∏:
- –í—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –Ω–∞–≤–∏—á–æ–∫
- –†—ñ–≤–µ–Ω—å –¥–æ—Å–≤—ñ–¥—É
- –õ–æ–∫–∞—Ü—ñ—è
- –¢–∏–ø –∑–∞–π–Ω—è—Ç–æ—Å—Ç—ñ
"""
        
        user_prompt = f"""
–†–ï–ó–Æ–ú–ï –ö–ê–ù–î–ò–î–ê–¢–ê:
{resume}

–í–ê–ö–ê–ù–°–Ü–Ø:
–ù–∞–∑–≤–∞: {job.title}
–ö–æ–º–ø–∞–Ω—ñ—è: {job.company}
–õ–æ–∫–∞—Ü—ñ—è: {job.location}
–ó–∞—Ä–ø–ª–∞—Ç–∞: {job.salary or '–ù–µ –≤–∫–∞–∑–∞–Ω–æ'}
–û–ø–∏—Å: {job.description[:1000]}

–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å —Ç–∞ –¥–∞–π –æ—Ü—ñ–Ω–∫—É.
"""
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await self.llm.ainvoke(messages)
        
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            result = json.loads(response.content)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            if not isinstance(result.get("score"), (int, float)):
                result["score"] = 5
            if not isinstance(result.get("should_apply"), bool):
                result["should_apply"] = result["score"] >= 7
                
            return result
        except json.JSONDecodeError:
            # –Ø–∫—â–æ LLM –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ –≤–∞–ª—ñ–¥–Ω–∏–π JSON
            print("‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ LLM")
            return {
                "score": 5,
                "reason": "–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏",
                "should_apply": False
            }
            
    def _save_report(self, state: AgentState):
        """–ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç —É —Ñ–∞–π–ª"""
        report = {
            "total_found": len(state["found_jobs"]),
            "total_analyzed": len(state["analyzed_jobs"]),
            "applied": [
                {
                    "title": job.title,
                    "company": job.company,
                    "url": job.url
                }
                for job in state["applied_jobs"]
            ],
            "rejected": [
                {
                    "title": job.title,
                    "company": job.company,
                    "score": next(
                        (a["score"] for a in state["analyzed_jobs"] if a["job"].url == job.url),
                        0
                    )
                }
                for job in state["rejected_jobs"]
            ]
        }
        
        with open("report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        print("\nüíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É report.json")
        
    async def run(self, resume_text: Optional[str] = None):
        """–ó–∞–ø—É—Å—Ç–∏—Ç–∏ –∞–≥–µ–Ω—Ç–∞"""
        initial_state = {
            "resume_text": resume_text or "",
            "search_keywords": config.SEARCH_KEYWORDS,
            "locations": config.LOCATIONS,
            "found_jobs": [],
            "analyzed_jobs": [],
            "applied_jobs": [],
            "rejected_jobs": [],
            "current_job_index": 0,
            "error": None,
            "scraper": None
        }
        
        print("ü§ñ –ó–∞–ø—É—Å–∫ AI –ê–≥–µ–Ω—Ç–∞ –¥–ª—è Work.ua")
        print("="*60)
        
        final_state = await self.graph.ainvoke(initial_state)
        
        return final_state


# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
async def test_agent():
    """–¢–µ—Å—Ç –∞–≥–µ–Ω—Ç–∞"""
    agent = WorkUAAgent()
    await agent.run()


if __name__ == "__main__":
    import asyncio
    print("üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è WorkUA Agent\n")
    asyncio.run(test_agent())
